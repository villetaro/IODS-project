#Chapter 4: Clustering and stuff'

We begin this week by analysing data from R's MASS package. The data contains information about The Boston area house-prices.

```{r, include=FALSE}

library(MASS)
library(corrplot)
library(tidyverse)
library(plotly)
library(dplyr)

```

```{r}
data("Boston")

str(Boston)

dim(Boston)

```

The data has 506 observations and 14 different variables. Next we plot the data to show the graphical overview and then we check how different variables compare to each other.

```{r}
summary(Boston)

```
This week we focus on the crim variable.

```{r}

cor_matrix<-cor(Boston) 


cor_matrix %>% round(digits = 2)
wb <- c("white","black") 

corrplot(cor_matrix, order="hclust", addrect=2, bg="gold2", col=wb)
 

```

This is a correlation plot which explain the correlations of different variables when being compared to each other. The bigger the spot the stronger the correlation.

One of the biggest one in the positive side is rad - tax. From the negative correlation side the biggest ones are dis - indus, dis - nox and dis - age.

Next we standardize the data.

```{r}

myboston_scaled <- scale(Boston)
summary(myboston_scaled)


```

After we standardized all the means are now 0.

```{r}
myboston_scaled <- as.data.frame(myboston_scaled)

```


```{r}
cutoffs <- quantile(myboston_scaled$crim)

labels <- c("low", "med_low", "med_high", "high")

crime_category <- cut(myboston_scaled$crim, breaks = cutoffs, include.lowest = TRUE, label = labels)

table(crime_category)


```

Now we drop the old crime rate variable from the dataset. Then we divide the dataset to train and test sets, so that 80% of the data belongs to the train set.


```{r}
myboston_scaled <- dplyr::select(myboston_scaled, -crim)
myboston_scaled <- data.frame(myboston_scaled, crime_category)
str(myboston_scaled)
```




```{r}
n <- nrow(myboston_scaled)
eighty_perc <- sample(n, size = n * 0.8)
train <- myboston_scaled[eighty_perc,]
test <- myboston_scaled[-eighty_perc,]

```

Now we fit the linear discriminant analysis on the train set. We use the categorical crime rate as the target variable and all the other variables in the dataset as predictor variables. This gives us an idea how crime rate interacts with the other variables.

```{r}
lda.fit <- lda(crime_category ~ ., data = train)
lda.fit

```


Now we draw the LDA biplot

```{r}
lda.arrows <- function(x, myscale = 0.5, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}
classes <- as.numeric(train$crime_category)
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 2)



```


Now we predict the classes with the LDA model on the test data. We can help our analysis by cross tabulating the results with the crime categories from the test set.

```{r}

correct_classes <- test$crime_category
test <- dplyr::select(test, -crime_category)

```
We see that in our model the first three groups (low, med_low and med_high) are very close to each other. The grouping is not that obvious and could use some more thought.

```{r}

lda.pred <- predict(lda.fit, newdata = test)
table(correct = correct_classes, predicted = lda.pred$class)

```

Let's see how well the model actually worked. I made a little observation during this analysis that the model changes everytime when I ran the code as some of the numbers come from random variables. As you read this the number may look a little different but I try to get the overall look.

The number of total observations
```{r}

total <- c(13+10+6+11+7+12+18+25)
total

```

We have 102 observations.

The number of correct predictions in this instance.
```{r}


correct <- c(13+18+11+25)
correct

```

We had 67 correctly predicted observations.

```{r}

wrong <- c(102-67)
wrong

```
And 35 of the observations were incorrectly predicted.

The accuracy of our model is the following:

```{r}
ratio <- c(correct/total)
ratio

```

At this time the model did ok but as I ran the code a couple of times the accuracy was quite different (0.66 as I write this).


Now we reload the Boston dataset and standardize it. Then we take a look at the distances between the variables.


```{r}
New_Boston <- Boston

str(New_Boston)

New_Boston_scaled <- scale(New_Boston) %>% as.data.frame()

str(New_Boston_scaled)
```

Now we calculate the distances.

```{r}
# Euclidean distance matrix using set.seed()
set.seed(123)
dist_eu <- dist(New_Boston_scaled)
summary(dist_eu)


```
This is based on the Euclidean measure. And above you see the summary of the findings. Now we can start the clustering with the amount of 10 clusters. 10 is a safe bet that nothing would go wrong, below I calculate the optimum amount of clusters.

```{r}

library(ggplot2)

km <-kmeans(dist_eu, centers=10)
pairs(New_Boston_scaled, col = km$cluster)

```


As there is 10 clusters at the moment it is really hard to see what actually happens in this plot. 

Because of this we want to optimize the amount of clusters so we don't have to guess it and then we can take a look at the plot again..


```{r}
k_max <- 10
wss <- sapply(1:k_max, function(k){kmeans(dist_eu, k)$tot.withinss})
plot(1:k_max, wss, type="b", xlab="Number of Clusters",
  ylab="Within groups sum of squares")

```

It seems that the optimal amount of clusters is 2. So we run the cluster analysis again and then take a look at the plot.


```{r}
km <-kmeans(dist_eu, centers=2)
pairs(New_Boston_scaled, col = km$cluster)

```

The above picture is quite similar compared to the one a moment ago but now the clusters are also included. The variables tax and  rad seem to work the best as in the differences are most clear compared to the plot without the clusters.

Next we perform LDA using the clusters as target classes. We include all the variables in the Boston data in the LDA model.

```{r}

km <-kmeans(dist_eu, centers = 3)
lda.fits <- lda(km$cluster~., data = myboston_scaled)
lda.fits

```

Now we visualize the results with a biplot and then analyze the results.
```{r}
plot(lda.fits, dimen = 2, col = classes) 
lda.arrows(lda.fits, myscale = 1)

```

Using 3 clusters for K-means analysis, the most influencial linear separators are nox and chas. This gives us an idea that they would be the best variables to predict new observations if those would appear.

The last thing to do is 3d modeling. It really tops this weeks project as it gives us a really clear model that we can also move 360 degrees. It helps us analyse the findings and understand the model.

```{r}
model_predictors <- dplyr::select(train, -crime_category)


dim(model_predictors)
dim(lda.fit$scaling)

matrix_product <- as.matrix(model_predictors) %*% lda.fit$scaling
matrix_product <- as.data.frame(matrix_product)


plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', color = classes, 
        colors=c("blue","yellow") )
```


