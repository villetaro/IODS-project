#Chapter 4: Clustering and stuff'
-> Lets begin!

We begin this week by analysing data from R's MASS package.

```{r}

library(MASS)
library(corrplot)
library(tidyverse)

data("Boston")

str(Boston)

dim(Boston)

```

The data has 506 observations and 14 different variables. Next we plot the data to show the graphical overview and then we check how different variables compare to each other.

```{r}
summary(Boston)

```
Tähän kommentti yleisesti datasta.


```{r}

cor_matrix<-cor(Boston) 


cor_matrix %>% round(digits = 2)
wb <- c("white","black") 

corrplot(cor_matrix, order="hclust", addrect=2, bg="gold2", col=wb)
 

```

Tähän kommentti muuttujien suhteesta toisiinsa.



```{r}

myboston_scaled <- scale(Boston)
summary(myboston_scaled)


```

After we standardized all the means are now 0.

```{r}
myboston_scaled <- as.data.frame(myboston_scaled)

```


```{r}
cutoffs <- quantile(myboston_scaled$crim)

labels <- c("low", "med_low", "med_high", "high")

crime_category <- cut(myboston_scaled$crim, breaks = cutoffs, include.lowest = TRUE, label = labels)

table(crime_category)


```


```{r}
myboston_scaled <- dplyr::select(myboston_scaled, -crim)
myboston_scaled <- data.frame(myboston_scaled, crime_category)
str(myboston_scaled)
```




```{r}
n <- nrow(myboston_scaled)
eighty_perc <- sample(n, size = n * 0.8)
train <- myboston_scaled[eighty_perc,]
test <- myboston_scaled[-eighty_perc,]

```


```{r}
lda.fit <- lda(crime_category ~ ., data = train)
lda.fit

```




```{r}
lda.arrows <- function(x, myscale = 0.5, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}
classes <- as.numeric(train$crime_category)
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 2)



```




```{r}

correct_classes <- test$crime_category
test <- dplyr::select(test, -crime_category)

```


```{r}

lda.pred <- predict(lda.fit, newdata = test)
table(correct = correct_classes, predicted = lda.pred$class)

```

Let's see how well the model actually worked. I made a little observation during this analysis that the model changes everytime when I ran the code as some of the numbers come from random variables. As you read this the number may look a little different but I try to get the overall look.

The number of total observations
```{r}

total <- c(13+10+6+11+7+12+18+25)
total

```

We have 102 observations.

The number of correct predictions in this instance.
```{r}


correct <- c(13+18+11+25)
correct

```

We had 67 correctly predicted observations.

```{r}

wrong <- c(102-67)
wrong

```
And 35 of the observations were incorrectly predicted.

The accuracy of our model is the following:

```{r}
ratio <- c(correct/total)
ratio

```

At this time the model did ok but as I ran the code a couple of times the accuracy was quite different (0.66 as I write this).


Now we reload the Boston dataset and standardize it. Then we take a look at the distances between the variables.


```{r}
New_Boston <- Boston

str(New_Boston)

New_Boston_scaled <- scale(New_Boston) %>% as.data.frame()

str(New_Boston_scaled)
```

Now we calculate the distances.

```{r}
# Euclidean distance matrix using set.seed()
set.seed(123)
dist_eu <- dist(New_Boston_scaled)
summary(dist_eu)


```
This is based on the Euclidean measure. And above you see the summary of the findings. Now we can start the clustering with the amount of 10 clusters.

```{r}

library(ggplot2)

km <-kmeans(dist_eu, centers=10)
pairs(New_Boston_scaled, col = km$cluster)

```

Now we want to optimize the amout of clusters so we don't have to guess it.


```{r}
k_max <- 10
wss <- sapply(1:k_max, function(k){kmeans(dist_eu, k)$tot.withinss})
plot(1:k_max, wss, type="b", xlab="Number of Clusters",
  ylab="Within groups sum of squares")

```

It seems that the optimal amount of clusters is 2. So we run the cluser analysis again and then take a look at the plot.


```{r}
km <-kmeans(dist_eu, centers=2)
pairs(New_Boston_scaled, col = km$cluster)

```


The last thing to do is 3d modeling

```{r}

library(plotly)
model_predictors <- dplyr::select(train, -crime_category)

# check the dimensions
dim(model_predictors)
dim(lda.fit$scaling)
# matrix multiplication
matrix_product <- as.matrix(model_predictors) %*% lda.fit$scaling
matrix_product <- as.data.frame(matrix_product)

labels <- c("low", "med_low", "med_high", "high")

plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', color = classes, 
        colors=c("blue","yellow") )
```


