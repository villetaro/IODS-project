#Chapter 5: Dimensionality reduction techniques

Lets begin the Chapter 5 by loading the data
```{r, include=FALSE}
library(ggplot2)
library(GGally)
library(MASS)
library(corrplot)
library(tidyverse)
library(plotly)
library(dplyr)
library(FactoMineR)
require(ggplot2)
library(tabplot)

```



```{r}
human <- read.table("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/human2.txt", header = T, sep=",")

dim(human)
str(human)

```


Next we look at the graphical overview of the data.


```{r}
p <- ggpairs(human, mapping = aes(col="blue", alpha=0.3), lower = list(combo = wrap("facethist", bins = 20)))

p
```

```{r}
summary(human)
```


```{r}
p2 <- ggplot(human, aes(x=GNI, y= Life.Exp)) + geom_point(col="deepskyblue1") + geom_smooth( col = "red2")

p2
```

```{r}
p3 <- ggplot(human, aes(x=GNI, y= Edu.Exp)) + geom_point(col="red2") + geom_smooth(col = "royalblue1")

p3
```

Next we perform principal component analysis (PCA) on the not standardized human data. 


```{r, warning=FALSE}
# perform principal component analysis (with the SVD method)
pca_human <- prcomp(human)

# draw a biplot of the principal component representation and the original variables
biplot(pca_human, choices = 1:2, cex = c(0.5, 1), col = c("black", "red2"), main="Biplot of the first two principal components for the unscaled data")

```

The picture doesn't look very good as we have not scaled the data. This is the reason why it is important to scale the data.


```{r}
human_std <- scale(human)

pca_human11 <- prcomp(human_std)

summary(pca_human11)

```


```{r}

biplot(pca_human11, choices = 1:2, cex = c(0.6, 1.2), col = c("black", "red2"),main="Biplot of the first two principal components for the scaled data")


```


Next we load the Tea dataset from Factominer package.
```{r}

data("tea")
dim(tea)
str(tea)

```


Lets continue tomorrow and try to find some idea to visualize 36 variables.
```{r}



```



